{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding Deep Neural Network Layers\n",
    "\n",
    "## 2D Convolutional Layers\n",
    "A **convolutional layer** convolves a **filter** (a coolection of **kernels**) with the input to produce an **activation or feature map**. Sometimes, a **bias** term is added to the output of the filter. Mathematically speaking, a convolution is the sum of element-wise multiplications of the filter over the input  in a sliding window fashion. In a convolutional neural network, the filter weights are learned. \n",
    "\n",
    "<img src=\"media/convolution.png\" alt=\"convolution example\" width=\"512\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 12, 12])\n",
      "torch.Size([1, 3, 16, 16])\n",
      "torch.Size([1, 3, 8, 8])\n",
      "torch.Size([1, 5, 16, 16])\n",
      "torch.Size([1, 3, 8, 8])\n",
      "torch.Size([1, 3, 8, 8])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "# Our randomly initialized input tensor\n",
    "# We can interpret it  as a batch of 1 16x16 RGB image (3 channels)\n",
    "x = torch.rand((1, 3, 16, 16))\n",
    "N, C, H, W = x.shape\n",
    "\n",
    "# KERNEL SIZE\n",
    "# This is the \"field of view\" of the convolution. This is also referred to as\n",
    "# the receptive field of the layer.\n",
    "y = torch.nn.Conv2d(\n",
    "    in_channels=C, \n",
    "    out_channels=C,\n",
    "    kernel_size=5)(x)\n",
    "print(y.shape) # torch.Size([1,3,12,12])\n",
    "\n",
    "# PADDING\n",
    "# Without padding, we can see that the ouput is smaller than the input. (If you \n",
    "# think about the sliding window, the edges of the input matrix are \"clipped\" \n",
    "# during convolution.) We can add padding to the matrix to get an output matrix \n",
    "# of the same size as the input (allow for centering of edge pixels to be in the \n",
    "# center of the filter)\n",
    "y = torch.nn.Conv2d(\n",
    "    in_channels=C, \n",
    "    out_channels=C,\n",
    "    kernel_size=5,\n",
    "    padding=2)(x)\n",
    "print(y.shape) # torch.Size([1,3,16,16])\n",
    "\n",
    "# STRIDE\n",
    "# We can also play around with striding to reduce the size of our feature map. \n",
    "# The stride specifies the \"step size\" of the filter as we slide through the \n",
    "# image. A stride of 2 means that we slide through the image 2 pixels at a time \n",
    "# (i.e., skipping a pixel.)\n",
    "y = torch.nn.Conv2d(\n",
    "    in_channels=C, \n",
    "    out_channels=C,\n",
    "    kernel_size=5,\n",
    "    padding=2,\n",
    "    stride=2)(x)\n",
    "print(y.shape) # torch.Size([1,3,8,8])\n",
    "\n",
    "# NUMBER OF KERNELS\n",
    "# The number of kernels in a filter will determine the number of channels in \n",
    "# your output feature map.\n",
    "y = torch.nn.Conv2d(\n",
    "    in_channels=C, \n",
    "    out_channels=5,\n",
    "    kernel_size=5,\n",
    "    padding=2)(x) \n",
    "print(y.shape) # torch.Size([1,5,16,16])\n",
    "\n",
    "# DILATION\n",
    "# Dilation essentially influates the kernel by inserting spaces between the\n",
    "# kernel elements. A dilation factor of l=2 means that we insert l-1 spaces \n",
    "# between the kernel elements. Dilation allows us to observe a larger receptive\n",
    "# field without adding additional costs (i.e. number of parameters).\n",
    "y = torch.nn.Conv2d(\n",
    "    in_channels=C, \n",
    "    out_channels=C,\n",
    "    kernel_size=5,\n",
    "    dilation=2)(x)\n",
    "print(y.shape) # torch.Size([1, 3, 8, 8])\n",
    "\n",
    "y = torch.nn.Conv2d(\n",
    "    in_channels=C, \n",
    "    out_channels=C,\n",
    "    kernel_size=9,\n",
    "    dilation=1)(x)\n",
    "print(y.shape) # torch.Size([1, 3, 8, 8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transposed Convolution\n",
    "\n",
    "This layer is also called **deconvolution** or **fractionally strided convolution**. Conceptually it's purpose is the opposite of a typical convolution layer, i.e. it is \"upsampling\" information instead of condensing information. This type of layer is used often in high-resolution image generation, or mapping low-dimensional feature space to higher-dimensional feature space as in autoencoders. \n",
    "\n",
    "Empirically, people have observed that using transposed convolution can lead to **checkerboard artifacts**. This is a result of uneven overlap when the filter size is not divisible by the stride. \n",
    "\n",
    "A normal convolution:\n",
    "\n",
    "<img src=\"media/convolution_computer.jpeg\" alt=\"convolution example\" width=\"512\"/>\n",
    "\n",
    "A transposed convolution:\n",
    "\n",
    "<img src=\"media/transpose_convolution.png\" alt=\"convolution example\" width=\"512\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 20, 20])\n"
     ]
    }
   ],
   "source": [
    "y = torch.nn.ConvTranspose2d(\n",
    "    in_channels=C, \n",
    "    out_channels=C, \n",
    "    kernel_size=5)(x)\n",
    "print(y.shape) #torch.Size([1, 3, 20, 20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pooling Layers\n",
    "\n",
    "A pooling layer reduces the spatial size of the input, thereby reducing the number of parameters and controlling overfitting. It is independently applied to every channel of the input, resizing it spatially using maximum or average operations.\n",
    "\n",
    "<img src=\"media/pooling.jpeg\" alt=\"convolution example\" width=\"512\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 8, 8])\n",
      "torch.Size([1, 3, 8, 8])\n"
     ]
    }
   ],
   "source": [
    "y = torch.nn.MaxPool2d(\n",
    "    kernel_size=2, \n",
    "    stride=2)(x)\n",
    "print(y.shape) #torch.Size([1, 3, 8, 8])\n",
    "\n",
    "y = torch.nn.AvgPool2d(\n",
    "    kernel_size=2, \n",
    "    stride=2)(x)\n",
    "print(y.shape) torch.Size([1, 3, 8, 8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization Layers\n",
    "\n",
    "A normalization layer normalizes activations during training and is typically placed between a convolutional layer and an activation layer. It learns two trainable parameters, (1) gamma, a standard deviation parameter and (2) beta, a mean parameter. It scales the input activations by these two parameters to force activations to be unit standard deviation and zero mean.\n",
    "\n",
    "**Batch normalization** normalizes over the minibatch, **layer normalization** normalizes across the features, and **instance normalization** normalizes across each channel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 16, 16])\n",
      "torch.Size([1, 3, 16, 16])\n",
      "torch.Size([1, 3, 16, 16])\n"
     ]
    }
   ],
   "source": [
    "y = torch.nn.BatchNorm2d(num_features=3)(x)\n",
    "print(y.shape) # torch.Size([1, 3, 16, 16])\n",
    "\n",
    "y = torch.nn.InstanceNorm2d(num_features=3)(x)\n",
    "print(y.shape) # torch.Size([1, 3, 16, 16])\n",
    "\n",
    "y = torch.nn.LayerNorm(x.size()[1:])(x)\n",
    "print(y.shape) #torch.Size([1, 3, 16, 16])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fully Connected Layers\n",
    "\n",
    "A fully connected layer has connections to all activations in the previous layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([48, 4])\n"
     ]
    }
   ],
   "source": [
    "x_flat = x.view(48,16) # torch.Size([48, 16])\n",
    "\n",
    "y = torch.nn.Linear(\n",
    "    in_features=16,\n",
    "    out_features=4)(x_flat)\n",
    "print(y.shape) # torch.Size([48, 4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nonlinear Layers\n",
    "\n",
    "Also known as **activation layers**.\n",
    "\n",
    "<img src=\"media/relu.png\" alt=\"convolution example\" width=\"512\"/>\n",
    "<img src=\"media/sigmoid.png\" alt=\"convolution example\" width=\"512\"/>\n",
    "<img src=\"media/tanh.png\" alt=\"convolution example\" width=\"512\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 16, 16])\n"
     ]
    }
   ],
   "source": [
    "y = torch.nn.ReLU()(x)\n",
    "y = torch.nn.Sigmoid()(x)\n",
    "y = torch.nn.Tanh()(x)\n",
    "print(y.shape) # torch.Size([1, 3, 16, 16])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Layers\n",
    "\n",
    "Recurrent layers are good for processing sequential information (e.g. speech, language) since they maintain a **hidden state** representing features from previous time steps. \n",
    "\n",
    "A **basic RNN cell** combines the input and previous hidden state to form a vector. That vector  goes through a nonlinear activation to output the new hidden state.\n",
    "\n",
    "<img src=\"media/rnn_cell.gif\" alt=\"convolution example\" width=\"512\"/>\n",
    "\n",
    "A downside of the basic RNN cell is short-term memory loss due to vanishing gradients-- RNNS have a hard time carrying information from earlier time steps to later ones. \n",
    "\n",
    "**Long Short-Term Memory (LSTM) cells** use three different gate types in order to control the flow of information to and from the **cell state**, which helps hold relevant information throughout the processing of the sequence. \n",
    "- The **forget gate** controls what part of the previous cell state will be kept.\n",
    "- The **input gate** controls what part of the new computed information will be added to the cell state.\n",
    "- The **out gate** controls what part of the cell state will exposed as the hidden state.\n",
    "\n",
    "**Gated Recurrent Units (GRUs)** are simpler than LSTM cells. They do not have a cell state and only have two gates, the **reset gate** and the **update gate**. \n",
    "- The update gate decides what information to keep from the hidden state.\n",
    "- The reset gate decides what part of the hidden state we use to compute the new proposal.\n",
    "<img src=\"media/lstm_gru.png\" alt=\"convolution example\" width=\"512\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input sequence of length 5, batch size 3, input size 16\n",
    "x = torch.rand((5, 3, 16))\n",
    "N, BS, L = x.shape\n",
    "\n",
    "# Hidden state of batch size 3, hidden size 10\n",
    "L_hidden = 10\n",
    "hx = torch.rand((BS, L_hidden))\n",
    "\n",
    "# Cell state of batch size 3, hidden size 10\n",
    "cx = torch.rand((BS, L_hidden))\n",
    "\n",
    "# Make the RNN cells\n",
    "rnn_cell = torch.nn.RNNCell(\n",
    "    input_size=L, \n",
    "    hidden_size=L_hidden)\n",
    "lstm_cell = torch.nn.LSTMCell(\n",
    "    input_size=L, \n",
    "    hidden_size=L_hidden)\n",
    "gru_cell = torch.nn.GRUCell(\n",
    "    input_size=L, \n",
    "    hidden_size=L_hidden)\n",
    "\n",
    "# Apply recurrent cells to input\n",
    "for i in range(N):\n",
    "    hx = rnn_cell(x[i], hx)\n",
    "for i in range(N):\n",
    "    hx, cx = lstm_cell(x[i], (hx, cx))\n",
    "for i in range(N):\n",
    "    hx = gru_cell(x[i], hx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "- https://towardsdatascience.com/a-comprehensive-introduction-to-different-types-of-convolutions-in-deep-learning-669281e58215\n",
    "- http://cs231n.github.io/convolutional-networks/\n",
    "- https://medium.com/techspace-usict/normalization-techniques-in-deep-neural-networks-9121bf100d8\n",
    "- https://jhui.github.io/2017/03/15/RNN-LSTM-GRU/\n",
    "- https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
